{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx1mlUzxsAAw",
        "outputId": "486d0290-424e-4ac4-f649-18eb10b06280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.10).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jagdishchakole1/mango-data?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 104M/104M [00:00<00:00, 133MB/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/jagdishchakole1/mango-data/versions/1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Download the dataset\n",
        "path = kagglehub.dataset_download(\"jagdishchakole1/mango-data\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Define data directories\n",
        "train_dir = os.path.join(path, \"Mango_leaf_disease1/train\")\n",
        "val_dir = os.path.join(path, \"Mango_leaf_disease1/val\")\n",
        "test_dir = os.path.join(path, \"Mango_leaf_disease1/test\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Just rescaling for validation and test sets\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Set up the generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(299, 299),  # InceptionV3 requires 299x299 input\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(299, 299),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(299, 299),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWN_ht_jsEa1",
        "outputId": "fc68a443-8fb5-4e2d-92da-4896d7363454"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2800 images belonging to 8 classes.\n",
            "Found 800 images belonging to 8 classes.\n",
            "Found 400 images belonging to 8 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of classes\n",
        "num_classes = len(train_generator.class_indices)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(\"Class mapping:\", train_generator.class_indices)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O2w5tMTsIqt",
        "outputId": "878fca52-31c1-4632-8bda-5f249b7b87ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 8\n",
            "Class mapping: {'Anthracnose': 0, 'Bacterial Canker': 1, 'Cutting Weevil': 2, 'Die Back': 3, 'Gall Midge': 4, 'Healthy': 5, 'Powdery Mildew': 6, 'Sooty Mould': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the base model from the pre-trained InceptionV3\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classification layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)  # Add dropout to prevent overfitting\n",
        "predictions = Dense(num_classes, activation='softmax')(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-woLcis_sInr",
        "outputId": "1f26a8b3-1eef-4289-89c0-c50d468508f1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "oebmX8TWsIlA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add callbacks for better training\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=3,\n",
        "    min_lr=1e-6\n",
        ")\n"
      ],
      "metadata": {
        "id": "My6pmOW8sIiX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=15,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // val_generator.batch_size,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "model.save('mango_disease_inception_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iabdqHJsTG8",
        "outputId": "7f4daaf6-2292-40b0-e8a2-dca646526a2d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 875ms/step - accuracy: 0.5977 - loss: 1.2596 - val_accuracy: 0.9175 - val_loss: 0.3209 - learning_rate: 1.0000e-04\n",
            "Epoch 2/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - accuracy: 0.8750 - loss: 0.3600 - learning_rate: 1.0000e-04\n",
            "Epoch 3/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
            "  current = self.get_monitor_value(logs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py:96: UserWarning: Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss,learning_rate.\n",
            "  callback.on_epoch_end(epoch, logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 678ms/step - accuracy: 0.9171 - loss: 0.2988 - val_accuracy: 0.9388 - val_loss: 0.2043 - learning_rate: 1.0000e-04\n",
            "Epoch 4/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267us/step - accuracy: 0.9688 - loss: 0.1351 - learning_rate: 1.0000e-04\n",
            "Epoch 5/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 692ms/step - accuracy: 0.9367 - loss: 0.2165 - val_accuracy: 0.9425 - val_loss: 0.1691 - learning_rate: 1.0000e-04\n",
            "Epoch 6/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9375 - loss: 0.1794 - learning_rate: 1.0000e-04\n",
            "Epoch 7/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 687ms/step - accuracy: 0.9521 - loss: 0.1616 - val_accuracy: 0.9513 - val_loss: 0.1405 - learning_rate: 1.0000e-04\n",
            "Epoch 8/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - accuracy: 0.9688 - loss: 0.1284 - learning_rate: 1.0000e-04\n",
            "Epoch 9/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 685ms/step - accuracy: 0.9596 - loss: 0.1446 - val_accuracy: 0.9563 - val_loss: 0.1237 - learning_rate: 1.0000e-04\n",
            "Epoch 10/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - accuracy: 1.0000 - loss: 0.0349 - learning_rate: 1.0000e-04\n",
            "Epoch 11/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 682ms/step - accuracy: 0.9650 - loss: 0.1163 - val_accuracy: 0.9725 - val_loss: 0.0968 - learning_rate: 1.0000e-04\n",
            "Epoch 12/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9375 - loss: 0.1412 - learning_rate: 1.0000e-04\n",
            "Epoch 13/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 670ms/step - accuracy: 0.9630 - loss: 0.1152 - val_accuracy: 0.9712 - val_loss: 0.0895 - learning_rate: 1.0000e-04\n",
            "Epoch 14/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251us/step - accuracy: 1.0000 - loss: 0.0692 - learning_rate: 1.0000e-04\n",
            "Epoch 15/15\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 658ms/step - accuracy: 0.9787 - loss: 0.0812 - val_accuracy: 0.9663 - val_loss: 0.0877 - learning_rate: 1.0000e-04\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 444ms/step - accuracy: 0.9730 - loss: 0.0822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.9650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VXkb2dLdsTEF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F61MgjeZsS9P"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}